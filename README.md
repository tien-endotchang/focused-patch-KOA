# DeepPatch


# Anatomically-Focused Patches for Lightweight and Explainable Knee OA Grading

This repository contains the official PyTorch implementation for the paper: **[Anatomically-Focused Patches for Lightweight and Explainable Knee OA Grading]**.

Our work introduces a novel framework for automated knee osteoarthritis (OA) grading from radiographs. By using **anatomically-focused patches** extracted from bone contours, our model achieves state-of-the-art performance with a fraction of the parameters of larger models, while also providing precise, explainable saliency maps.


---

## Installation

To set up the required environment, we provide a Conda environment file.

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/w2051200021/DeepPatch.git
    cd DeepPatch
    ```

2.  **Create the Conda environment:**
    ```bash
    conda env create -f DeepPatch_env.yml
    ```

3.  **Activate the environment:**
    ```bash
    conda activate DeepPatch
    ```

---

## Workflow & Usage

The process is divided into three main steps: **Data Preprocessing**, **Training**, and **Inference**.

### 1. Data Preprocessing

This step generates the "bag of patches" from the raw data.

**Requirements:**
*   A directory containing knee radiographs in DICOM format.
*   A directory containing landmark files (`.pts`) for each radiograph, generated by an anatomical landmark detector (e.g., BoneFinder).
*   A file containing the Kellgren-Lawrence (KL) grades for each radiograph (e.g., `kxr_sq_bu00.txt` from the OAI dataset).

**Instructions:**
1.  Open and run the `shape_patch_kl.ipynb` notebook.
2.  Make sure to set the paths to your DICOMs, landmark files, and KL grade file within the notebook.
3.  This notebook will generate and save two crucial files:
    *   `id_shapeLR_V00.npz`: Contains patient IDs, landmark coordinates, and KL grades.
    *   `./data/knee_patches_patient_grouped_16_100.h5`: The main dataset file containing the processed patches, ready for training.

### 2. Model Training

This step trains the MIL model on the preprocessed data.

**Instructions:**
1.  Open and run the `train.ipynb` notebook.
2.  The notebook will automatically load the `.h5` data file created in the previous step.
3.  Upon completion, the notebook will:
    *   Save the best model weights (based on validation set performance) to the `./model_checkpoints/` directory.
    *   Generate predictions (`test_pred.npz`) and attention scores (`test_att_scores.npz`) for the test set and save them in the `./inference/` directory for the next step.

*Note: For convenience, we have already included our pre-trained model weights in the `./train/model_checkpoints/` folder.*

### 3. Inference and Visualization

This step uses the trained model to make predictions and generate the saliency maps presented in our paper.

**Instructions:**
1.  Open and run the `inference.ipynb` notebook.
2.  This notebook uses the pre-computed predictions and attention scores from the `./inference/` directory to:
    *   Calculate evaluation metrics (Accuracy, F1-score, Kappa).
    *   Generate the confusion matrix (`cm.eps`, `cm.png`).
    *   Produce the final attention-weighted saliency maps for visualization.

---

